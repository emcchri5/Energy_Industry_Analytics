---
title: "ENE 434 Lab 6 Assignment"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#loading in data (lab stuff)

```{r}
if (!require('pacman')) install.packages('pacman')
library(pacman)
p_load(tidyverse, fpp3, lubridate)
```

```{r}
power_df = read_csv("https://raw.githubusercontent.com/emcchri5/codebank/main/power_df.csv")
colnames(power_df)[23] = 'ets_price'
power_df$date = yearmonth(power_df$date)
power_ts = tsibble(power_df, index = date)
power_ts = power_ts %>% mutate(
  DK1 = DK1/1000
)
```
```{r}
#scenarios

#no change in ets price
scen1 = new_data(power_ts, 12) %>% 
  mutate(ets_price = rep(power_ts$ets_price[128],12))

#constant increase of .5EUR
scen2 = new_data(power_ts, 12) %>%
  mutate(
    ets_price = rep(power_ts$ets_price[128],12) + cumsum(rep(.5,12))
         )
```

#Assignment
##Question 1
In a dynamic regression model, it may make sense to include lagged variables as exogenous regressors. In the model of DK1 prices, include both contemporaneous and lagged carbon permit prices. How does this change your model? (You may want to read Ch 10.6 in fpp3).
```{r}
armax3 = power_ts %>% fill_gaps() %>% model(
  modWithEts = ARIMA(DK1 ~ ets_price + pdq(2,1,0)),
  mod_ets_lagged = ARIMA(DK1 ~ ets_price + pdq(2,1,0) +
                lag(ets_price) + lag(ets_price, 2) + lag(ets_price, 3)))
glance(armax3) %>% arrange(AICc) #better fit!!
armax2 = power_ts %>% fill_gaps() %>% model(
  modWithEts = ARIMA(DK1 ~ ets_price + pdq(2,1,0)),
  modWOutEts = ARIMA(DK1 ~ pdq(2,1,0))
  )
```

```{r}
armax3 %>% 
  select(modWithEts) %>%
  gg_tsresiduals()
armax3 %>% 
  select(mod_ets_lagged) %>%
  gg_tsresiduals()
```

```{r}
fcast3 = armax2 %>% select(modWithEts) %>% forecast(new_data=scen1)
fcast4 = armax2 %>% select(modWithEts) %>% forecast(new_data=scen2)
fcast5 = armax3 %>% select(mod_ets_lagged) %>% forecast(new_data=scen1)
fcast6 = armax3 %>% select(mod_ets_lagged) %>% forecast(new_data=scen2)
```

```{r}
fcast3 %>% autoplot(power_ts) #constant carbon prices, ets model (no lags)
fcast4 %>% autoplot(power_ts) #increasing carbon prices, ets model
fcast5 %>% autoplot(power_ts) #constant carbon prices, lagged model
fcast6 %>% autoplot(power_ts) #increasing carbon prices, lagged model
```

##Question 2
From ENTSOE-E or statnett, download hourly consumption data for Norway for 2017 and 2018. Join this with the 2019 data in order to create one long time series for Norwegian consumption. Then model the seasonality in the data (at monthly, weakly and daily level), with fourier terms.
###importing data
```{r}
cons_2017 <- read_csv('https://raw.githubusercontent.com/emcchri5/codebank/main/NO_Energy_Cons_2017.csv')
cons_2018 <- read_csv('https://raw.githubusercontent.com/emcchri5/codebank/main/NO_Energy_Cons_2018.csv')
cons = read_csv2("http://jmaurit.github.io/analytics/labs/data/consumption-no-areas_2019_hourly.csv")
```

###getting hour columns
```{r}
cons_2017 <- cons_2017 %>%
  separate('Time(Local)', sep = ' ', into=c('date','time','timezone')) %>%
  separate(time, sep = ':', into=c('hour', 'minute', 'second'))
cons_2018 <- cons_2018 %>%
   separate('Time(Local)', sep = ' ', into=c('date','time','timezone')) %>%
  separate(time, sep = ':', into=c('hour', 'minute', 'second'))
cons <- cons %>% separate(Hours, sep = '-', into=c('start','end'))
```

###converting hour to numeric
```{r}
cons_2017 <- cons_2017 %>%
  mutate(hour = as.numeric(hour))
cons_2018 <- cons_2018 %>%
  mutate(hour = as.numeric(hour))
```

###binding 2017 and 2018 data (easy part)
```{r}
cons_20178 <- rbind(cons_2017, cons_2018)
cons_20178 <- cons_20178 %>%
  select(date, hour, Consumption)
cons_2019 <- cons %>% 
  select(Date, start, NO)
```

###wrangling date values to be parallel
```{r}
cons_20178 <- cons_20178 %>%
  rename('cons' = 'Consumption')
cons_20178$date <- gsub('\\.', '/', cons_20178$date)
cons_2019$time <- gsub('\\s+', '', cons$start)
cons_2019$hour <- as.numeric(cons_2019$time)
cons_2019 <- cons_2019 %>%
  select(Date, hour, NO) %>%
  rename('date' = 'Date', 'cons' = 'NO')
```
###binding datasets 2017/2018 and 2019, fixing data to tsibble
```{r}
cons_total <- rbind(cons_20178, cons_2019)

cons_total["period"] = dmy_h(paste(cons_total$date, cons_total$hour))

#check for NA's
cons_total[!complete.cases(cons_total), ]
#replace NA's
cons_total[["cons"]][cons_total$period==as_datetime("2019-03-31 02:00:00")] = cons_total[["cons"]][cons_total$period==as_datetime("2019-03-31 01:00:00")]
#check for duplicates
duplicates(cons_total)
#run this 3 times  until duplicates(cons_total is gone)
dupRow = duplicates(cons_total)[2,]
cons_total = cons_total %>% rows_delete(dupRow, by=c("period", 'cons'))
duplicates(cons_total) #should be an empty 0x4 dataframe
cons_total_ts <- cons_total %>%
  tsibble(index=period) %>%
  fill_gaps()
```

```{r}
smod3 = cons_total_ts %>% model(
  fmod7 = ARIMA(cons ~ fourier(period = 24, K = 1) + fourier(period = 24*7, K = 1) + fourier(period = 24*30, K = 1) + fourier (period = 24*365, K=1) + PDQ(0,0,0))
)
glance(smod3)
```

```{r}
forecast <- smod3 %>%
  forecast(h = 24*365)
forecast %>% autoplot(cons_total_ts)
```

##Question 3
Create a VAR model for consumption and prices in 2019 using Danish data (You can find it on ENTSOE_E or at the Danish TSO’s energy data site. Create a 30 day forecast. Load in actual data for january 2020–how does your forecast look? Include wind power in Denmark as a variable. How does this affect the model and forecast?
#data wrangling, getting dan consumption ready
```{r}
DAN <- read.csv('https://raw.githubusercontent.com/emcchri5/codebank/main/Total%20Load%20Denmark.csv')
DAN <- DAN %>%
  rename('total_load' = 'Actual.Total.Load..MW....CTA.DK', 'datetime' = 'Time..CET.CEST.') %>%
  select(datetime, total_load)
DAN <- DAN %>%  separate('datetime', sep = ' - ', into=c('start','end')) %>%
  separate('start', sep = ' ', into = c('date', 'time')) %>%
  select(-end)
  
DAN
```
#data wrangling, getting dan prices together
```{r}
DAN_price <- read.csv('https://raw.githubusercontent.com/emcchri5/codebank/main/EL_prices_2019.csv')
DAN_price <- DAN_price %>% select("ï..HourUTC","PriceArea","SpotPriceEUR") %>%
  rename(Time = 'ï..HourUTC', price = "SpotPriceEUR") %>%
  filter(PriceArea == 'DK1') %>%
  separate('Time', sep = 'T', into=c('date','time')) %>%
  separate('time', sep = "[+]", into=c('tid','useless')) %>%
  select(-useless,-PriceArea) %>%
  separate('tid', sep = ':', into= c('hour', 'minute','second'))
DAN_price$time <- paste(DAN_price$hour, DAN_price$minute, sep= ":")
DAN_price <- DAN_price %>%
  select(-hour,-minute,-second)
DAN_price
DAN_price <- DAN_price %>%
  separate('date', sep = '-', into = c('year', 'month', 'day'))
DAN_price
DAN_price$dizzy <- paste(DAN_price$day, DAN_price$month, sep = '.')
DAN_price$date <- paste(DAN_price$dizzy, DAN_price$year, sep = '.')
DAN_price <- DAN_price %>%
  select(-year,-month,-day,-dizzy)
DAN_price %>%
  arrange(by_group = date)
```
#data wrangling... merging sets and tsibble stuff
```{r}
DAN_data <- left_join(DAN_price, DAN)
DAN_data <- DAN_data %>% 
  separate('time',sep = ':', into = c('hour','minute'))
DAN_data$date <- gsub('\\.', '-', DAN_data$date)
DAN_data["period"] = dmy_h(paste(DAN_data$date, DAN_data$hour))
DAN_data[!complete.cases(DAN_data), ]
DAN_data[["total_load"]][DAN_data$period==as_datetime("2019-03-31 02:00:00")] = DAN_data[["total_load"]][DAN_data$period==as_datetime("2019-03-31 01:00:00")]
dupRow = duplicates(DAN_data)[2,]
DAN_data = DAN_data %>% rows_delete(dupRow, by=c("period", 'total_load'))
DAN_data <- DAN_data %>%
  select(-hour, -minute) %>%
  tsibble(index='period')
DAN_data 
```

```{r}
DAN_data_diff <- DAN_data %>% mutate( 
  dprice = difference(price), #can't log price since it's negative
  log_dcons = difference(log(total_load))
)
DAN_data_diff<- na.omit(DAN_data_diff) %>%
  select(period, price, total_load,dprice,log_dcons)
DAN_data_diff %>% select(dprice) %>% autoplot()
DAN_data_diff %>% select(log_dcons) %>% autoplot()
```

```{r}
varmod = DAN_data_diff %>%
  model(
    mod1 = VAR(vars(dprice, log_dcons))
  )
varmod2 = DAN_data_diff %>%
  model(
    mod2 = VAR(vars(price, total_load))
  )
varmod %>% report()
varmod2 %>% report()
```
It's such a horrible model. The forecasting brough me back here to check. Where did we go wrong?

```{r}
varmod %>%
  augment() %>%
  ACF(.innov) %>%
  autoplot() #failed to account for daily seasonality (or 12 hours for that matter)
```
```{r}
DAN_data_diff <- DAN_data_diff %>%
  mutate('periodd' = 'period') %>%
  separate('period', sep = ' ', into=c('date','time')) %>%
  mutate(date = as_date(date))
DAN_data_diff
varmod %>%
  forecast(h=365*24) %>%
  autoplot(DAN_data_diff)
varmod2 %>%
  forecast(h=30*24) %>%
  autoplot(DAN_data_diff)
```
Why are the forecasts straight lines? This seem absolutely wrong. The expected difference is the mean, pretty much. 
 
```{r}
DAN_2020 <- read.csv('https://raw.githubusercontent.com/emcchri5/codebank/main/consumption%202020.csv')
DAN_2020
DAN_2020 <- DAN_2020 %>%
  rename('total_load' = 'Actual.Total.Load..MW....BZN.DK1', 'datetime' = 'Time..CET.CEST.') %>%
  select(datetime, total_load)
DAN_2020 <- DAN_2020 %>%  separate('datetime', sep = ' - ', into=c('start','end')) %>%
  separate('start', sep = ' ', into = c('date', 'time')) %>%
  select(-end)

DAN_2020 <- DAN_2020 %>%
  separate('time', sep = ':', into=c('hour','minute'))

DAN_2020["period"] = dmy_h(paste(DAN_2020$date, DAN_2020$hour))
DAN_2020 <- DAN_2020 %>%
  select(-hour, -minute, -date) 
dupRow = duplicates(DAN_2020)[2,]
DAN_2020 = DAN_2020 %>% rows_delete(dupRow, by=c("period", 'total_load'))
DAN_2020 <- DAN_2020 %>%
  tsibble(index = 'period')
DAN_2020 %>% select(total_load) %>% autoplot() #I can't perform filters on this tsibble format... also our 95% confidence interval didn't work for January. I think it's safe to say we've failed this question. Where did we go wrong? Why is our prediction flat? 
```
 


